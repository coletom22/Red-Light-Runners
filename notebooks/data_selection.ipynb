{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligently select training data (ISD) that reflects upcoming raw batches\n",
    "1. Create embeddings for the upcoming batch of data (1000 images) using `ResNet50`\n",
    "2. Run `top_k` on all embeddings, storing matches in `selected_imgs`\n",
    "3. Iterate over `selected_imgs` and add to training stage\n",
    "4. Train new model and view performance on upcoming data\n",
    "\n",
    "### Notes\n",
    "Because we are simply going off ResNet's embeddings, it will be important that in the future we check to make sure our data contains a fair representation of classes.\n",
    "\n",
    "For this notebook, there will be two models trained to compare the performance:\n",
    "1. __Control__: trained on a random subset of our data equal in length to the other model\n",
    "2. **ISD**: trained on only the `selected_imgs` from our embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "load_dotenv()\n",
    "pc = Pinecone(api_key=os.getenv(\"PC_API_KEY\"))\n",
    "index_name = 'rlr-embeddings'\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View ResNet architecture\n",
    "Take note of the final layer, we will remove the final output layer because the layer prior will act as our embeddings layer <br>\n",
    "`(fc): Linear(in_features=2048, out_features=1000, bias=True) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# there are 2048 in_features, the dimensions of our embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing the final layer with `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),    # ResNet 50 expects image sizes of 224x224\n",
    "    transforms.ToTensor(),            # converts PIL image / NumPy array to tensor\n",
    "    transforms.Normalize(             \n",
    "        mean = [0.485, 0.456, 0.406], # mean for each channel (RGB)\n",
    "        std = [0.229, 0.224, 0.225]   # std for each channel\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define upcoming batch (1000 or length of remaining frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img_dir = \"../data/images/frames\"\n",
    "\n",
    "raw_imgs = os.listdir(raw_img_dir)\n",
    "if len(raw_imgs) < 1000:\n",
    "    num_imgs = len(raw_imgs)\n",
    "else:\n",
    "    num_imgs = 1000\n",
    "\n",
    "print(f\"Raw images: {len(raw_imgs)}, Number of images: {num_imgs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "#### Iterate over our images:\n",
    "1. Transform\n",
    "2. Create Embedding\n",
    "3. Add to list of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    \n",
    "    # open current image\n",
    "    image_path = os.path.join(raw_img_dir, raw_imgs[i])\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    img_tensor = transform(img).unsqueeze(0) # add transformations from cell above\n",
    "\n",
    "    # torch.no_grad does not calculate the gradients to reduce memory usage / increase speed\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).squeeze(-1).squeeze(-1) # remove the last two dimensions of the tensor\n",
    "\n",
    "    embedding = embedding / torch.norm(embedding, p=2) #L2 Normalization\n",
    "    embedding = embedding.numpy().tolist()[0]          # convert to 1 dimensional list\n",
    "\n",
    "    embeddings.append(embedding) # add embedding to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Pinecone\n",
    "Store `match['id']` (which are the names of the images) and find the top **50** nearest neighbors.<br>\n",
    "Use a set to automatically eliminate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = set()\n",
    "all_scores = []\n",
    "for embedding in embeddings:\n",
    "    query_response = index.query(\n",
    "        vector=embedding,\n",
    "        top_k = 50,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    scores = [match['score'] for match in query_response['matches']]\n",
    "    all_scores.extend(scores)\n",
    "    knns = [match['id'] for match in query_response['matches']]\n",
    "    for neighbor in knns:\n",
    "        selected_imgs.add(neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_imgs))\n",
    "print(all_scores[0])\n",
    "min_score = 1\n",
    "for score in all_scores:\n",
    "    min_score = min(min_score, score)\n",
    "print(min_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the average embedding to retreive the top 10000 NN\n",
    "Naturally, there will be a lot of overlap between nearest neighbors of the upcoming dataset so we need to find a strategy for expanding the set. This method will increase the number of training examples while also including niche examples (notice how the total number of training examples exceeds 10000. Those examples come from nearest neighbors that are not within the 10000 of the average embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding = np.mean(embeddings, axis=0).tolist()\n",
    "print(avg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = index.query(\n",
    "    vector=avg_embedding,\n",
    "    top_k = 10000,\n",
    "    include_metadata=False\n",
    ")\n",
    "scores = [match['score'] for match in query_response['matches']]\n",
    "knns = [match['id'] for match in query_response['matches']]\n",
    "min_score = 1\n",
    "for score in scores:\n",
    "    min_score = min(min_score, score)\n",
    "print(min_score)\n",
    "for neighbor in knns:\n",
    "        selected_imgs.add(neighbor)\n",
    "print(len(selected_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organize new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for img in selected_imgs:\n",
    "    orig_dir = \"../data/images/processed\"\n",
    "    new_dir = \"../data/images/temp\"\n",
    "    shutil.move(os.path.join(orig_dir, img), os.path.join(new_dir, img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "\n",
    "import data_management\n",
    "img_train_dir = '../data/model_data/images/train'\n",
    "img_val_dir = '../data/model_data/images/validation'\n",
    "\n",
    "label_train_dir = '../data/model_data/labels/train'\n",
    "label_val_dir = '../data/model_data/labels/validation'\n",
    "\n",
    "data_management.train_val_split(\"../data/images/temp\", \"../data/labels/formatted\", img_train_dir, label_train_dir, img_val_dir, label_val_dir, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model on ISD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from datetime import datetime\n",
    "model_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  \n",
    "model = YOLO(\"yolov8n.yaml\")\n",
    "results = model.train(\n",
    "    data = \"../SLD.yaml\", \n",
    "    epochs=30, \n",
    "    imgsz=768, \n",
    "    device=0, \n",
    "    project=\"../runs\", \n",
    "    name=f\"{model_name}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import video_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/current_assistant/2025-03-27_17-25-00.pt\"\n",
    "video_path = \"../data/videos/processed/20250222_154541M.mp4\"\n",
    "video_out = \"../images/result_videos/testing_ISD_compare.mp4\"\n",
    "video_processing.predict_video(video_path, video_out, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare performance of recently trained model vs ISD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import screeninfo\n",
    "\n",
    "def display_side_by_side(video_path_1, video_path_2):\n",
    "    # Open the two videos\n",
    "    cap1 = cv2.VideoCapture(video_path_1)\n",
    "    cap2 = cv2.VideoCapture(video_path_2)\n",
    "\n",
    "    if not cap1.isOpened() or not cap2.isOpened():\n",
    "        print(\"Error: Could not open one of the videos.\")\n",
    "        return\n",
    "\n",
    "    # Get the available screens\n",
    "    monitors = screeninfo.get_monitors()\n",
    "\n",
    "    # If there is more than one monitor, set the second monitor for full-screen\n",
    "    if len(monitors) > 1:\n",
    "        second_monitor = monitors[1]\n",
    "    else:\n",
    "        print(\"Only one monitor detected, displaying on the primary monitor.\")\n",
    "        second_monitor = monitors[0]  # Fall back to the first monitor\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from each video\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        # If either video is over, break the loop\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "\n",
    "        # Resize frames to have the same height for side-by-side display\n",
    "        height = min(frame1.shape[0], frame2.shape[0])\n",
    "        frame1_resized = cv2.resize(frame1, (int(frame1.shape[1] * height / frame1.shape[0]), height))\n",
    "        frame2_resized = cv2.resize(frame2, (int(frame2.shape[1] * height / frame2.shape[0]), height))\n",
    "        # Add text label to the top of each frame\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        frame1_labeled = cv2.putText(frame1_resized, video_path_1, (10, 30), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        frame2_labeled = cv2.putText(frame2_resized, video_path_2, (10, 30), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        # Stack the frames side by side\n",
    "        combined_frame = cv2.hconcat([frame1_labeled, frame2_labeled])\n",
    "\n",
    "        # Move the window to the second monitor\n",
    "        cv2.namedWindow(\"Final Frame\", cv2.WND_PROP_FULLSCREEN)\n",
    "        cv2.moveWindow(\"Final Frame\", second_monitor.x, second_monitor.y)\n",
    "\n",
    "        # Resize the final frame to fit the second monitor\n",
    "        second_monitor_width = second_monitor.width\n",
    "        second_monitor_height = second_monitor.height\n",
    "        combined_frame_resized = cv2.resize(combined_frame, (second_monitor_width, second_monitor_height))\n",
    "\n",
    "        # Show the final frame on the second monitor\n",
    "        cv2.imshow(\"Final Frame\", combined_frame_resized)\n",
    "\n",
    "        # Set the window to full-screen mode\n",
    "        cv2.setWindowProperty(\"Final Frame\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "        # Wait for a key press to proceed to the next frame, or ESC to terminate\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key == 27:  # ESC key to exit\n",
    "            break\n",
    "\n",
    "    # Move the window to the second monitor\n",
    "    cv2.namedWindow(\"Final Frame\", cv2.WND_PROP_FULLSCREEN)\n",
    "    cv2.moveWindow(\"Final Frame\", second_monitor.x, second_monitor.y)\n",
    "\n",
    "    # Resize the final frame to fit the second monitor\n",
    "    second_monitor_width = second_monitor.width\n",
    "    second_monitor_height = second_monitor.height\n",
    "    combined_frame_resized = cv2.resize(combined_frame, (second_monitor_width, second_monitor_height))\n",
    "\n",
    "    # Show the final frame on the second monitor\n",
    "    cv2.imshow(\"Final Frame\", combined_frame_resized)\n",
    "\n",
    "    # Set the window to full-screen mode\n",
    "    cv2.setWindowProperty(\"Final Frame\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "    # Wait for the ESC key to close the window\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    # Release video captures and close any OpenCV windows\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "video_path_1 = '../images/result_videos/testing_ISD.mp4'\n",
    "video_path_2 = '../images/result_videos/testing_ISD_compare.mp4'\n",
    "display_side_by_side(video_path_1, video_path_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
